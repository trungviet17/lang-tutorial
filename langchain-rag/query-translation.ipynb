{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Translation \n",
    "- Vấn đề ảnh hưởng tới hiệu quả của quá trình trích xuất thông tin đó là việc người dùng cung cấp thông tin mù mờ, không rõ ràng. Việc câu hỏi không rõ ràng dẫn tới trích xuất thông tin cũng không rõ ràng -> ảnh hưởng tới việc hiểu của LLM và kết quả của LLM \n",
    "\n",
    "- Một cách để giải quyết vấn đề này là thực hiện một vài phương pháp tiền xử lý đầu vào của người dùng (xử lý câu hỏi của người dùng) nhằm giúp câu hỏi trở nên rõ ràng, nhiều góc độ hơn. \n",
    "\n",
    "- Query Translation là một chuỗi các hành động xử lý trong quá trình embedding giữa query và docs để đảm bảo thông tin được trích xuất luôn được đảm bảo tính chính xác.   \n",
    "\n",
    "- Các cách để chuyển đổi câu query : \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/general-approaches-transform-question.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Indexing \n",
    "import bs4 \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader thông qua nguồn là trang web\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://en.wikipedia.org/wiki/Faker_(gamer)\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"mw-body-content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "\n",
    "# khởi tạo vectorstore lưu trữ thông tin \n",
    "vectorstore = Chroma.from_documents(documents= splits, \n",
    "        embedding= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",  google_api_key = GOOGLE_API_KEY))\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-written \n",
    "- Với các câu hỏi khó hiểu do cách diễn đạt của người dùng (không trừu tượng quá mà cũng không quá cụ thể), có hai cách thức chính để xử lý với dạng câu hỏi kiểu này đó là Multi-query và RAG-Fusion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-query \n",
    "1. **Ý tưởng**: Ta có thể giải quyết vấn đề trên thông qua việc chuyển đổi, viết lại câu query của người dùng theo nhiều các khác nhau. Việc này đựa trên niềm tin rằng ngôn từ mà người dùng xử dụng sau khi embedding không thể trích xuất ra từ ngữ một cách chính xác. Việc viết lại sẽ cho câu query nhiều góc độ quan sát hơn, thông tin được đa dạng hơn\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/multi-query.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is to generate five \\ndifferent versions of the given user question to retrieve relevant documents from a vector \\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search. \\nProvide these alternative questions separated by newlines. Original question: {question}'))])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-pro-latest', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000205A8E8D2D0>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x00000205A8F9A110>, default_metadata=())\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cài đặt multi-query ## \n",
    "\n",
    "# viết lại tập câu hỏi thông qua mô hình ngôn ngữ lớn. \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspective\n",
    "    | ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0, api_key = GOOGLE_API_KEY)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "generate_queries\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "INPUT : \n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: How many World Championship titles has Faker won in his League of Legends career?\n",
    "\n",
    "RENDER OUTPUT : \n",
    "Here are five alternative ways to phrase the question \"How many World Championship titles has Faker won in his League of Legends career?\" to enhance search results in a vector database:\n",
    "\n",
    "1. What is the total number of League of Legends World Championships won by Faker?\n",
    "2. How many times has Faker been crowned a League of Legends World Champion?\n",
    "3. What is Faker's World Championship win count in professional League of Legends?\n",
    "4. In the history of League of Legends, how many World Championship titles belong to Faker?\n",
    "5.  List the years Faker won the League of Legends World Championship. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(docs : list[list]): \n",
    "    # lấy ra thông tin duy nhất được trích xuất từ câu hỏi đầu vào \n",
    "    flattened_docs = [dumps(doc) for sublist in docs for doc in sublist]\n",
    "\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "question = \"How many World Championship titles has Faker won in his League of Legends career?\"\n",
    "\n",
    "# tạo ra retrieval chain, với đầu vào là câu một số câu query được tạo ra truóc đó -> lấy doc liên quan thông qua retriever -> tìm docs duy nhất trong số đó. \n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content='During the 2023 LCK Spring Split, Faker achieved another LCK record. On January 20, 2023, in a match against KT Rolster, Faker surpassed Kang \"Gorilla\" Beom-hyeon to claim the record for the most career assists in the history of the LCK at 4,137.[77] On July 2, in the 2023 LCK Summer Split, Faker was sidelined due to an arm injury. His absence from competitive play extended for a duration of four weeks. During this time, T1\\'s record fell from 6–2 to 7–9. Faker returned to the starting roster on August 2, helping the team defeat the Kwangdong Freecs.[78][79] Winning their last game of the season, T1 finished with a 9–9 record, securing the fifth seed in the LCK Summer playoffs.[80] T1 reached the LCK Summer finals, where they lost to Gen.G. Finishing with the most championship points in the LCK, the team qualified for the 2023 World Championship, marking Faker\\'s eighth Worlds appearance.[81][82] Faker won his fourth Worlds title on November 19, 2023, after T1 defeated Weibo Gaming in the finals. With the win, Faker became the first person to win four World Championship titles and, at age 27, the oldest player to win one.[83] At the 2023 LCK Awards ceremony in'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content='^ Alexander, Jem (October 27, 2019). \"The best League of Legends players: top pros from across the globe\". PCGamesN. Archived from the original on March 6, 2023. Retrieved March 5, 2023.\\n\\n^ Gubert, Jéssica (December 20, 2022). \"League of Legends stars with the most World Championships\". Dot Esports. Archived from the original on March 6, 2023. Retrieved March 5, 2023.\\n\\n^ Dyer, Mitch (November 2, 2015). \"Worlds: How SK Telecom T1 Won the 2015 League of Legends Championship\". IGN. Archived from the original on March 6, 2023. Retrieved March 5, 2023.\\n\\n^ East, Tom (October 12, 2016). \"Watch Faker\\'s strange LoL Worlds celebration\". Red Bull. Archived from the original on March 6, 2023. Retrieved March 5, 2023.\\n\\n^ Gu, Rachel (November 1, 2015). \"League of Legends team becomes first ever two time World Champions\". GameSpot. Archived from the original on March 6, 2023. Retrieved March 5, 2023.\\n\\n^ a b Bencomo, Brian (October 31, 2022). \"A legendary career: Faker\\'s results at Worlds and MSI\". Nerd Street. Archived from the original on March 6, 2023. Retrieved March 5, 2023.'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content='Faker celebrating after winning the 2023 World Championship'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content='Faker competing at the 2015 League of Legends World Championship\\nFaker celebrating after winning the 2015 World Championship'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content='defeating Griffin in the playoff finals, giving Faker his eighth LCK title.[48] At the 2019 World Championship, Faker became the first player to reach 100 international wins, after SKT defeated Splyce in the quarterfinals.[49] However, SKT were defeated in the semifinals by G2 Esports, marking the first time that Faker had been eliminated from Worlds in the knockout stage.[50]'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Faker_(gamer)'}, page_content=\"Faker's individual achievements include accolades such as a World Championship Most Valuable Player (MVP) award, an MSI MVP award, two LCK season MVP awards, an LCK Finals MVP award, an LCK Player of the Year award, an LCK Mid Laner of the Year award, and two LCK First All-Pro Team designations. He holds several LCK records, including being the first player to reach 1,000, 2,000, and 3,000 kills, the first to have earned 5,000 assists, the first to have played 900 games, and the first to have won 600 games in the LCK. Faker also holds the record for the most kills in World Championship matches and was the first player to surpass 100 World Championship wins. His accomplishments have earned him recognition as the Best Esports Athlete at The Game Awards in 2017 and 2023, PC Player of the Year at the Esports Awards in 2023, and he was named to the Forbes 30 Under 30 list in Asia Entertainment & Sports in 2019. Additionally, he was inducted into the ESL Hall of Fame in the same year. He was also chosen as the inaugural inductee for the LoL Esports Hall of Legends, being officially announced by Riot Games in May 2024.\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion \n",
    "- Một nhược điểm của việc sử dụng multi-query đó là với số lượng query sinh ra sẽ có một số lượng lớn context được lựa chọn, làm cho input của mô hình LLM trở lên vô cùng lớn -> có thể khiến mô hình hoạt động kém hoặc không hoạt động. Bên cạnh đó, multi-query chỉ làm đa dạng context, nó có thể lấy ra những context không đúng hoặc ít liên quan tới câu hỏi. Để cải thiện vấn đề này, người ta dã thêm một module với nhiệm vụ là lọc bỏ những context không liên quan, và lựa chọn số lượng context giới hạn \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/rag-fusion.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROMPT ## \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Generate multi-query from query \n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# chain sinh ra nhiều câu query \n",
    "generate_query = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0, api_key = GOOGLE_API_KEY)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tính rank của document ## \n",
    "from langchain.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Hàm tính rank-fusion của các document, trong đó, nó nhận chuỗi các chuỗi document (được lấy ra từ mỗi query trước đó) \n",
    "        đã được đánh rank trước đó và tham số mịn k để cho phương trình RRF để tránh tính toán 1/0\"\"\"\n",
    "    \n",
    "    # khởi tạo fused_score dict dể lưu điểm của các document một cách duy nhất.\n",
    "    fused_scores = {}\n",
    "\n",
    "    # duyệt qua các list doc trong result \n",
    "    for docs in results:\n",
    "        # Duyệt qua các doc trong list doc và rank của nó \n",
    "        for rank, doc in enumerate(docs):\n",
    "            # chuyển đổi doc thành string và là key của fused_scores\n",
    "            doc_str = dumps(doc)\n",
    "            # nếu doc chưa tồn tại trong dictionary -> nó có score là 0 \n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # trích xuất thông tin của scores trước đó \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update score sử dụng RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Ranking lại toàn bộ result thông qua việc sort theo thứ tự giảm dần dựa vào fused_score\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace_Trung\\AI\\simple-rag\\rag\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Sinh query và trích xuất thông tinh đã được đánh rank ## \n",
    "question = \"How many World Championship titles has Faker won in his League of Legends career?\"\n",
    "retrieval_chain_rag_fusion = generate_query | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final pipeline with multi-query \n",
    "\n",
    "from operator import itemgetter \n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) \n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0, api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# xây dựng final chain thực hiện liên tục sinh multi-query -> lấy context phù hợp -> prompt -> llm -> ouput \n",
    "final_rag_chain = (\n",
    "    {'context': retrieval_chain, \n",
    "     'question' : itemgetter(\"question\")}\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# trích xuất câu hỏi từ chain \n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposition\n",
    "- Với một câu query với lượng thông tin quá lớn, việc trích xuất thông tin có thể không chính xác, hoặc quá chung chung. Để xác nhận điều này, thay vì viết lại câu theo nhiều các khác nhau, người ta có thể chia nhỏ thông tin câu query thành nhiều phần và sử dụng phương pháp trích xuất song song hoặc đệ quy để trích xuất thông tin từ đó. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinh các sub-question từ câu hỏi gốc thông qua prompt \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are 3 search queries related to the components of an LLM-powered autonomous agent system:',\n",
       " '',\n",
       " '1. **\"LLM agent architecture memory perception action loop\"** (This query targets the core structural elements and how they interact)',\n",
       " '2. **\"Tools and APIs for building autonomous agents with large language models\"** (This focuses on the practical building blocks and available resources)',\n",
       " '3. **\"Role of planning, reasoning, and learning in LLM-based autonomous agents\"** (This query dives into the cognitive capabilities that LLMs bring to such systems) ',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# llm \n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0, api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# chain \n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Answering Approach\n",
    "- Với hướng tiếp cận sử dụng đệ quy, sau khi tách các câu hỏi, thông qua vectorstore, dữ liệu dần được trích xuất, và lần lượt đưa chúng qua mô hình ngôn ngữ lớn. Thông tin trả lời của câu sau sẽ được tách ra thành thông tin trả lời của câu trước đó. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/recur-decompo-query.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt sinh câu trả lời thông qua việc lấy đệ quy \n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer): \n",
    "    ''' chuẩn hóa câu hỏi - câu trả lời '''\n",
    "\n",
    "    format = f'Question: {question}\\nAnswer: {answer}\\n'\n",
    "    return format.strip()\n",
    "\n",
    "\n",
    "# llm \n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0, api_key = GOOGLE_API_KEY)\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions: \n",
    "\n",
    "    rag_chain = (\n",
    "        {'context': itemgetter(\"question\") | retriever, # trích xuất thông tin từ question  \n",
    "         'question': itemgetter(\"question\"), # câu hỏi được generate ở trên \n",
    "         'q_a_pairs': itemgetter(\"q_a_pairs\")} # tạo cặp câu hỏi q-a-pair \n",
    "        | decomposition_prompt\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # trích xuất câu hỏi với từng cặp câu hỏi - trả lời trước đó. \n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})    \n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Answering Approach\n",
    "- Cách thức này chuyển đổi các câu hỏi - câu trả lời một cách song song, câu hỏi được sinh ra được mô hinh trả lời song song và sau đó được tổng hợp thành một câu hỏi chính\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/para-decompo-query.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trả lời lần lượt các câu hỏi \n",
    "from langchain import hub \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "prompt_rag = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "\n",
    "def retrieval_and_rag(question, prompt_rag, subquestion_generator_chain): \n",
    "    '''RAG trên từng tập câu hỏi con '''\n",
    "\n",
    "    # trích xuất câu hỏi con \n",
    "    sub_questions = subquestion_generator_chain.invoke({'question': question})\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # trích xuất câu trả lời từ tập câu hỏi \n",
    "    for sub_quest in sub_questions: \n",
    "\n",
    "        # trích xuất docs liên quan tới sub_quest \n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_quest)\n",
    "\n",
    "        ans = (prompt_rag | llm | StrOutputParser()).invoke({'context': retrieved_docs, 'question': sub_quest})\n",
    "\n",
    "        result.append(ans)\n",
    "\n",
    "    return result, sub_questions\n",
    "\n",
    "# sinh câu hỏi - câu trả lời \n",
    "answers, questions = retrieval_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tổng hợp câu hỏi - câu trả lời \n",
    "\n",
    "def format_qa_pairs(questions, answers): \n",
    "    ''' chuyển đổi đoạn question và answer '''\n",
    "    formatted_string = \"\"\n",
    "\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start = 1): \n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "\n",
    "    return formatted_string\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "final_answer = final_rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-back \n",
    "- Đề giúp câu trả lời được sinh ra có tính logic cao hơn, mở rộng context cho một câu hỏi của người dùng, người ta đã sử dụng phương pháp step-back (làm câu hỏi trở lên trừu tượng, khái quát hóa hơn). Context có thể được lấy trực tiếp từ query người dùng kết hợp với context được lấy thông qua câu hỏi trừu tượng hơn giúp cho câu trả lời được sinh ra hiệu quả hơn. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/step-back-query.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinh câu hỏi step-back thông qua few-shot learning \n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# ví dụ \n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    }, \n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# prompt template cho ví dụ \n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"), \n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# khởi tạo few-shot learning prompt \n",
    "fewshot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt, \n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "\n",
    "# khởi tạo prompt \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        fewshot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain sinh step-back question \n",
    "step_back_generation_chain = (prompt | llm | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ví dụ về sinh step-back question \n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "step_back_generation_chain.invoke({\"question\": question}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# final prompt \n",
    "response_prompt  = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# khởi tạo final-chain \n",
    "final_chain = (\n",
    "    {   \n",
    "        # trích xuất context thông thường thông qua question \n",
    "        \"normal_context\": RunnableLambda(lambda x: x['question']) | retriever, \n",
    "        \"step_back_context\": step_back_generation_chain | retriever, # trích xuất thông tin liên quan tới step back \n",
    "        \"question\": lambda x: x['question'], \n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# chạy thử chain \n",
    "final_chain.invoke({\"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE \n",
    "- Đôi khi việc tìm hay so sánh độ tương đồng giữa những câu hỏi của người dùng và context phù hợp có thể không chính xác. Đều này đến từ việc document được chunks thành các đoạn lớn hơn nhiều so với input đầu vào, ma trận của nó thường dense. Để khắc phục điều đó, phương pháp HyDE chuyển đổi các query của người dùng thành các passage giống với định dạng của docs và sử dụng nó để trích xuất thông tin hiểu quả hơn\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/hyde-transform.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng prompt để sinh ra docs từ question (hypothetical doc)\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "generate_hypo_docs_chain = (\n",
    "    prompt_hyde \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# test thử \n",
    "generate_hypo_docs_chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trích xuất docs liên quan tới template \n",
    "retrieval_hype_chain = generate_hypo_docs_chain | retriever\n",
    "re_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "\n",
    "# Rag \n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "hype_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_hype_chain = (\n",
    "    prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_hype_chain.invoke({\"context\": re_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tham khảo \n",
    "[Query Translation for RAG (Retrieval Augmented Generation)Applications](https://raghunaathan.medium.com/query-translation-for-rag-retrieval-augmented-generation-applications-46d74bff8f07)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
