{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giới thiệu \n",
    "- Notebook này là phần được ghi chú lại trong quá học khóa học [rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch?tab=readme-ov-file) về việc xây dựng mô hình RAG từ đầu\n",
    "- Trong phần này chủ yếu giới thiệu về cách xây dựng một pipeline cơ bản nhất cho một hệ thống RAG, pipeline bao gồm 3 phần chính như sau: \n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/basic-image.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong đó : \n",
    "1. Indexing: Phần dữ liệu nền tảng sẽ được thu thập và tách, sau đó được indexing vào trong một vector database\n",
    "2. Retrieval: phần này xử lý dữ liệu người dùng (user query) sẽ được index vào trong vector database để tìm kiếm được k chunks liên quan nhất tới query \n",
    "3. Generation : phần này chuyển đổi yêu cầu của người dùng và lượng dữ liệu của đã được trích xuất tù phần retrieval để tạo ra response với trả lời câu hỏi của người dùng dựa trên kiến thức được trích xuất.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triển khai Basic pipeline\n",
    "- Triển khai một pipeline của RAG dựa trên : \n",
    "    1. Mô hình LLM và Embedding : Gemini\n",
    "    2. Dữ liệu indexing : [wiki - Faker](https://en.wikipedia.org/wiki/Faker_(gamer)) \n",
    "    3. Trích xuất để trả lời câu hỏi \"How many World Championship titles has Faker won in his League of Legends career?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\WorkSpace_Trung\\AI\\simple-rag\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import thư viện cần thiết \n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/document-loading.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document sẽ được load và lưu trữ vào trong một vector database, việc này giúp tìm kiếm và truy xuất thông tin một cách hiệu quả hơn \n",
    "- Langchain hỗ trợ cho nhiều nguồn dữ liệu được chuyển đổi thông qua module [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "- Một số nguồn thường dùng là dữ liệu trực tiếp từ trang web hoặc các file định dạng PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader thông qua nguồn là một file pdf \n",
    "loader = PyPDFLoader(\n",
    "    file_path = \"data\\TTHCM1.pdf\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# loader thông qua nguồn là trang web\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://en.wikipedia.org/wiki/Faker_(gamer)\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"mw-body-content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi dữ liệu được load từ các nguồn trên, để trích xuất hoạt động hiệu quả và nhanh chóng hơn, dữ liệu sẽ được chia thành từng đoạn nhỏ theo chunk_size và chunk_overlap. Dữ liệu sẽ được chia nhỏ cho tới khi vừa dủ hoặc nhỏ hơn với chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split chia đoạn document được load\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để máy tính, các mô hình có thể hiểu được ngữ cảnh, và ngôn ngữ, nó cần được biểu diễn dưới dạng số. Các kí tự được mã hóa này được gọi là Token. Token giống như một chuỗi các kí tự trong đoạn text và sau đó được mã hóa trở thành một biểu diễn đưới dạng số "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\"\n",
    "\n",
    "# đếm số lượng token của một câu \n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mô hình embedding giúp chuyển đối các kí tự dạng chữ, biểu diễn chúng dưới dạng số, chuỗi số, giúp mô hình có thể hiểu được. Các biểu diễn này thông qua mô hình vẫn có thể nắm bắt được các mối quan hệ giữa các thành phần có trong câu. Sau khi chuyển đổi này, các mô hình còn hỗ trợ việc tính toán độ tương đồng giữa các câu, token thông qua tính toán các vector\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/embedding-model.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",  google_api_key = os.environ['GOOGLE_API_KEY'] )\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kết thúc quá trình xử lý dữ liệu, các vector sẽ được lưu trữ vào trong một hệ cơ sở dữ liệu dành riêng. Hệ cơ sở này thực hiện việc chuyển đổi các chuỗi đã được split từ trước dó thông qua mô hình embedding để lưu trữ chúng dưới dạng các vector. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/preprocessing-pipeline.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding đoạn dữ liệu vào trong vector database thông qua mô hình Gemini\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",  google_api_key = GOOGLE_API_KEY ))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval \n",
    "- Indexing bản chất là quá trình xử lý dữ liệu và chuyển đổi thông qua mô hình embedding. \n",
    "- Retrieval là quá trình trích xuất dữ liệu, thông tin liên quan với thông tin đầu vào từ trong vector database. \n",
    "- Có rất nhiều thuật toán tìm kiếm thông tin gần nhất trong một vector database (cosine similarity)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/retrieval-algo.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set đầu ra của kết quả retrieval luôn là 1 \n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# tìm kiếm doc liên quan tới câu hỏi \n",
    "docs = retriever.get_relevant_documents(\"How many World Championship titles has Faker won in his League of Legends career?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation \n",
    "- Tiếp tục với pipeline ở ban đầu, sau khi một số lượng document được trích xuất ra liên quan tới câu hỏi, tổng hợp chúng sẽ được đi qua một mẫu prompt và một LLM để trả lời câu hỏi của người dùng.\n",
    "- Luồng thông tin được trích xuất từ hệ cơ sở dữ liệu và câu hỏi \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/adding-generation.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace_Trung\\AI\\simple-rag\\rag\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt theo nguồn có sẵn \n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt được tự làm theo mẫu\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# khởi tạo llm \n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0.2, api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# tạo một chain từ bộ dữ liệu \n",
    "chain = prompt | llm \n",
    "\n",
    "# trả lời câu hỏi thông qua trích xuất \n",
    "chain.invoke({\n",
    "    \"context\": docs, \n",
    "    \"question\": \"How many World Championship titles has Faker won in his League of Legends career?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain tổng quát hơn \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "full_chain = (\n",
    "    {'context': docs, 'question': RunnablePassthrough()}\n",
    "    | prompt  \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "full_chain.invoke(\"How many World Championship titles has Faker won in his League of Legends career?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
