{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing \n",
    "- Trong mô hình RAG cơ bản, quá trình indexing là quá trình chuyển đổi một bộ dữ liệu, tài liệu lớn, được chia nhỏ thành các phần, được index vào trong các vectordatabase. Trong phần này sẽ trình bày các kĩ thuật xử lý trong quá trình indexing nhằm nâng cao hiệu quả của trích xuất. Từ việc xử lý với các đoạn văn bản dài, các phân chia (chunking) sao cho phù hợp,.... \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/indexing.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup các biến môi trường\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-representation indexing \n",
    "- Thông thường, dữ liệu thường được phần chia và split thành nhiều phần khác nhau với size được fix sẵn. Tuy nhiên khi trích xuất thông tin các phần dữ liệu được fix sẵn này tỏ ra hoạt động không hiệu quả. Từ đó, theo nghiên cứu, người ta thay vì trực tiếp embedding các phần dữ liệu nhỏ này thì người ta thường embedding proposition của nó theo [bài báo](https://arxiv.org/pdf/2312.06648.pdf) và trích xuất toàn bộ tài liệu liên quan vào trong các LLM để xử lý. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/multi-presentation-indexing.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "- Các này làm cho dữ liệu trích xuất ra không chỉ có chữ mà còn có thể là hình ảnh, dạng bảng, .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dựa vào pipeline trên, để sinh proposition, đoạn các đoạn thông tin sẽ được chia nhỏ theo từng đoạn dài (theo trang hoặc nhiều trang) -> tóm tắt chúng và embedding nó vào trong vector store.\n",
    "- Sau dó, các câu hỏi của người dùng sẽ được embedding vào vector store và so sánh các câu hỏi từ đó. Kết quả đầu ra sẽ là toàn bộ phần document từ việc so sánh các vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuẩn bị dữ liệu \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo sinh proposition index \n",
    "\n",
    "from langchain_core.documents import Document \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-pro-latest', temperature = 0.2, api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# sinh tóm tắt cho một trang của document \n",
    "\n",
    "summarize_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "summaries = summarize_chain.batch(docs, {\"max_concurrency\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "import uuid \n",
    "# init vector store \n",
    "vectorstore = Chroma(collection_name=\"summaries\", \n",
    "                     embedding_function= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",  google_api_key = GOOGLE_API_KEY))\n",
    "\n",
    "# xây dựng index giữa tóm tắt và document \n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# retriever \n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore = vectorstore, \n",
    "    byte_store = store, \n",
    "    id_key = id_key \n",
    ")\n",
    "doc_id = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# link doc with summaries \n",
    "summary_docs = [\n",
    "    Document(page_content = s, metadata = {id_key : doc_id[i]}) for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# add \n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_id, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trich xuat cau summary gan nhat trong vctdb \n",
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query, k = 1)\n",
    "\n",
    "# trich xuat tai lieu lien quan \n",
    "retrieval_docs = retriever.get_releavant_documents(query, n_results = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR \n",
    "- Với một số câu hỏi, yêu cầu LLM cần phải hiểu rõ bộ tài liệu thì mới có thể trả lời được -> cần có long context. Tuy nhiên, việc xử lý long context với các LLM hiện đại khá đơn giản nhưng có độ trễ cao và tương đối tốn kém. Do đó một giải pháp đó là người ta đã thiết kế mô hình RAPTOR \n",
    "- Kiến trúc của RAPTOR được thiết kế theo dạng hình cây, với node lá là document gốc, các document sẽ lần lượt được nhóm lại (clustering) sau đó tóm tắt thành những văn bản tổng quát hơn và lại lần lượt được embedding vào trong cơ sở dữ liệu \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/raptor.png\" alt=\"basic-pipeline\" width=\"400\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
