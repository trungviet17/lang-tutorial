{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing \n",
    "- Trên thực tế, dữ liệu có thể được lưu trữ ở nhiều nguồn khác nhau với các nguồn dữ liệu chứa các thông tin riêng biệt. Việc thiết kế hệ thống RAG nhắm query tới đúng nguồn dữ liệu hay phân loại các câu query phù hợp với từng dạng prompt, cần phải có một module phần loại, phân chia được gọi là routing. Dựa và 2 cách thức sử dụng ở trên mà người ta có thể phân loại thành 2 cách thức là Logical routing (phân loại để lựa chọn nguồn dữ liệu phù hợp) và Semantic routing (phân loại để lấy câu prompt thích hợp )\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/routing.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "- Bản chất router giống như một câu lệnh if/else giúp câu query được lựa chọn vào luồng phù hợp. Các cách thực hiện router có thể biểu diễn dưới hình sau: \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/routers.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân loại dựa theo cách thức \n",
    "Dựa và 2 cách thức sử dụng ở trên mà người ta có thể phân loại thành 2 cách thức là Logical routing (phân loại để lựa chọn nguồn dữ liệu phù hợp) và Sematic routing (phân loại để lấy câu prompt thích hợp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical routing \n",
    "- Là phương pháp lựa chọn các nguồn dữ liệu phù hợp dựa vào query của người dùng\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/logical-routing.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xây dựng các retrievers có sẵn \n",
    "from typing import Literal \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "\n",
    "# xây dựng base-model minh hoạ cho các cơ sở dữ liệu \n",
    "class RouteQuery(BaseModel): \n",
    "    \"\"\" Router để người dùng tìm kiếm source phù hợp \"\"\"\n",
    "\n",
    "    datasource : Literal[\"python\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "\n",
    "# xây dựng LLm với function calling, đảm bảo dữ liệu đầu ra \n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-flash-001', temperature = 0, api_key=GOOGLE_API_KEY)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# xây dựng prompt phân loại \n",
    "template = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template), \n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# xây dựng chain \n",
    "router_chain = prompt | structured_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ví dụ minh họa \n",
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Router\n",
    "- Thực tế, với tùy vào input của người dùng mà các câu prompt có thể được thiết kế khác nhau để tối ưu việc trích xuất thông tin. Vấn đề đặt ra là làm sao để biết query của người dùng phù hợp với prompt nào, người ta đã giới thiệu phương pháp router đề giải quyết vấn đề trên. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../doc/image/semantic-routing.png\" alt=\"basic-pipeline\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "- Thông qua việc embedding cả prompt lẫn query từ người dùng, sử dụng các thuật toán tính độ tương đồng, có thể lựa chọn ra prompt gần nhất với câu hỏi của người đùng. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape. It's formed when a massive star collapses at the end of its life. \\n\\nImagine a giant star, much bigger than our sun, running out of fuel. Without the outward pressure from nuclear fusion, gravity takes over, crushing the star's core into an incredibly dense point called a singularity.  \\n\\nThe singularity's gravity is so intense that it warps the fabric of spacetime around it, creating a region called the event horizon.  Anything that crosses the event horizon is trapped forever, doomed to spiral into the singularity. \\n\\nThink of it like a cosmic drain, pulling everything in, but with no way out. \\n\\nWhile we can't directly see black holes, we can observe their effects on surrounding matter and light.  They're fascinating objects that continue to challenge our understanding of gravity and the universe. \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# tạo tập prompt bất kì \n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "\n",
    "# embedding prompt \n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",  google_api_key = GOOGLE_API_KEY)\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embedding = embedding.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "\n",
    "# lựa chọn prompt thông quan qua query đã được embedding \n",
    "def prompt_router(input): \n",
    "    # embedding query \n",
    "    query_embedding = embedding.embed_query(input[\"query\"])\n",
    "    # tính toán similarity \n",
    "    similarity = cosine_similarity([query_embedding], prompt_embedding)[0]\n",
    "    most_similarity = prompt_templates[similarity.argmax()]\n",
    "\n",
    "    return PromptTemplate.from_template(most_similarity)\n",
    "\n",
    "\n",
    "# khởi tạo chain \n",
    "semantic_chain = (\n",
    "    {\"query\" : RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# trích xuất kết quả \n",
    "semantic_chain.invoke(\"What's a black hole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
